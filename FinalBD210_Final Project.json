{"paragraphs":[{"text":"%dep\r\n//z.load(\"com.crealytics:spark-excel_2.11:0.11.1\")\r\nz.load(\"org.vegas-viz:vegas-spark_2.11:0.3.9\")","user":"anonymous","dateUpdated":"2019-03-07T03:17:07+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@6fd91b24\n"}]},"apps":[],"jobName":"paragraph_1551652108855_576360759","id":"20190303-222828_295810940","dateCreated":"2019-03-03T22:28:28+0000","dateStarted":"2019-03-07T03:17:08+0000","dateFinished":"2019-03-07T03:17:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4179"},{"text":"%md\n#README|Deliverables\n\n###Objective:\nMy goal was to pull in weather and storm events over time in order to gain an understanding of which places in the US have the least risk of dangerous storms occuring or which generally have bad weather (based on historical data in both cases), and once those locations are determine, figure out which industries are located in those same places in order to determine roughly what types of jobs might be available in the safer and more pleasant climates to live in. \n\n###Approach:\n1. Identify sources of data (listed below)\n2. Each of the sources of data were csvs (through the storm-events data was gzip'd) or txt files.  I wrote a simple shell script to download the files (there were many, broken out by year, so I didn't want to download each by hand) to my local machine.  See the section \"Scripts not run from Zeppelin\" below.\n    _Issue:_ Initially, I wanted to use the XLS files available on the BLS site -> they were laid out in a much more human-readable fashion and I wouldn't have to combine in the industry code file.  I did find that I could load in individual excel files if I used the library \"com.crealytics:spark-excel_2.11:0.11.1\", but I could not figure out an easy way to read in all the files in Scala in Zeppelin.  Reading in multiple CSVs is easy with the wildcard (*), so I ended up re-working the project with those.  [Here](https://github.com/crealytics/spark-excel) is the link to the repo for that library which I was using as a reference.\n    _Question:_ I wasn't sure which approach was better (I wasn't able to successfully test it) -> Should I unzip the files prior to adding them into HDFS, or should I unzip them as I read them into memory? \n3. _Issue:_ Ran out of space in HDFS-> Per some instructions laid out by Jerry, mount the data directory onto HDFS (steps written out below)\n4. Transfer the files to the /data/ directory on the VM using WINSCP. \n5. Load the files to HDFS on our VMs\n    I chose HDFS because it seemed to fit the data I was uploading -> normalized, column-based data whose size fit the criteria (not too small, especially once combined). I wasn't intending to do anything too interactive with the data either. Also, I chose HDFS because it was the most convenient and what I was most comfortable with based on my experience so far in the class (working with it since week 2).\n6. Read the CSV files in via a Zeppelin notebook.  I chose to use Zeppelin because my goal was to both practice using Scala on Spark as well as to effectively communicate my steps in the project, including the option to be able to visualize the data\n7. Save the CSVs as parquets -> the files took a long time to load via CSV, but load times with parquet were much much faster (reduced from tens of minutes to less than 5 secs)\n8. See the rest of the notebook for the remaining steps.\n9. NOTE: [This file provides a relatively good explanation for the fields for the BLS data](https://data.bls.gov/cew/doc/layouts/csv_quarterly_layout.htm)\n\n###Data Sources:\n1. [The storm events details files from the NOAA](https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/) provided data about stormevents since the 1950s through today in the US.  \n2. [BLS Quarterly census of employment and wages](https://www.bls.gov/cew/datatoc.htm#NAICS_BASED) (singlefile CSVs), provided a view of jobs by industry code in the US.  \n3. To provide a human-readable view of the industry codes from the BLS, I also joined in their industry code file, found [here](https://data.bls.gov/cew/doc/titles/industry/industry_titles.csv).  A description of this file can be found [here](https://data.bls.gov/cew/doc/titles/industry/industry_titles.htm)\n4. This ended up not being necessary: In order to combine the two sources of data, I needed to incorporate a forth reference to \"translate\" between the code for location in the stormevents file and the code for area in the employment file.  I found this data [also on the census site from the US government](https://www2.census.gov/geo/docs/reference/codes/files/national_county.txt)\n\n\n\n\n###Scripts not run from Zeppelin:\n```sh\n##Download all the stormevents data from the NOAA\n##Run on local windows machine\nwget --no-clobber --convert-links --random-wait -r -p -E -e robots=off -U mozilla https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/ \n```\n```sh\n##Download all the jobs data from the BLS and unzip them\n##Run on local windows machine\nfor i in {1990..2018}; do echo \"https://data.bls.gov/cew/data/files/$i/csv/\"\"$i\"\"_qtrly_singlefile.zip\";\nwget \"https://data.bls.gov/cew/data/files/$i/csv/\"\"$i\"\"_qtrly_singlefile.zip\"; \nunzip \"$i\"\"_qtrly_singlefile.zip\"; rm \"$i\"\"_qtrly_singlefile.zip\"; done\n###Note: this I initially ran for the period from 1990 to 2018, but due to the size of the resulting files, I ended up only using data between 2011 and 2018\n####Though there was space on disk for most (if not all) of these files, the time it took to read in the data to the notebook was getting frustratingly long.\n```\n```sh\n##This is the list of commands I ran per Jerry's instructions on how to mount the /data/ drive to HDFS\nmkdir -p /data/hadoop/hdfs/data  \nchown -R hdfs:hadoop /data/hadoop/hdfs/data #to match the ownership Hadoop expects\n##Next, I edited the HDFS configuration in Ambari to add, in a comma-separated list, the newly created directory to \"Datanode directories\" in Services / HDFS / Configs, alongside the original directory HDFS was using \n##Finally, I restarted HDFS from Ambari\n```\n```sh\n##Copy the files from the VM to HDFS (done for both for the BLS data and the stormevents)\nhadoop fs -copyFromLocal /data/singlefiles /tmp/\n```\n\nIf you are having trouble with the code, please provide a detailed description of what you are trying to do in order to demonstrate understanding of the course material\n \n\n#Presentation\n\nStudents will provide a brief (2-5 min) presentation of their project in the last class session to share with classmates.\n\nIf you need to be absent please contact instructor so that appropriate arrangements can be made.  \n\nStudents can optionally record a video in advance of the last class and post to YouTube or similar for viewing, and then answer any questions live.\n\nExample Outline\n1. Introduce the data being used\n1. Describe the objective of the project\n1. Walk through approach, providing justifications of decisions made\n1. For example, don’t just say you stored your results in Redis, say why you stored your results in Redis\n1. Include code where appropriate\n1. Share results\n1. Issues you had, if any E.g. Something you would have wanted to do but couldn’t get working or what you would like to do with more time\n Ran into issue with data size, which seems counterintuitive.  Had to remove a number of files and only analyze 2010 through 2 quarters into 2018\n1. Answer any questions","user":"anonymous","dateUpdated":"2019-03-09T23:59:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>README|Deliverables</h1>\n<h3>Objective:</h3>\n<p>My goal was to pull in weather and storm events over time in order to gain an understanding of which places in the US have the least risk of dangerous storms occuring or which generally have bad weather (based on historical data in both cases), and once those locations are determine, figure out which industries are located in those same places in order to determine roughly what types of jobs might be available in the safer and more pleasant climates to live in.</p>\n<h3>Approach:</h3>\n<ol>\n<li>Identify sources of data (listed below)</li>\n<li>Each of the sources of data were csvs (through the storm-events data was gzip'd) or txt files.  I wrote a simple shell script to download the files (there were many, broken out by year, so I didn't want to download each by hand) to my local machine.  See the section &ldquo;Scripts not run from Zeppelin&rdquo; below.\n<br  /><em>Issue:</em> Initially, I wanted to use the XLS files available on the BLS site -> they were laid out in a much more human-readable fashion and I wouldn't have to combine in the industry code file.  I did find that I could load in individual excel files if I used the library &ldquo;com.crealytics:spark-excel_2.11:0.11.1&rdquo;, but I could not figure out an easy way to read in all the files in Scala in Zeppelin.  Reading in multiple CSVs is easy with the wildcard (*), so I ended up re-working the project with those.  <a href=\"https://github.com/crealytics/spark-excel\">Here</a> is the link to the repo for that library which I was using as a reference.\n<br  /><em>Question:</em> I wasn't sure which approach was better (I wasn't able to successfully test it) -> Should I unzip the files prior to adding them into HDFS, or should I unzip them as I read them into memory?</li>\n<li><em>Issue:</em> Ran out of space in HDFS-> Per some instructions laid out by Jerry, mount the data directory onto HDFS (steps written out below)</li>\n<li>Transfer the files to the /data/ directory on the VM using WINSCP.</li>\n<li>Load the files to HDFS on our VMs\n<br  />I chose HDFS because it seemed to fit the data I was uploading -> normalized, column-based data whose size fit the criteria (not too small, especially once combined). I wasn't intending to do anything too interactive with the data either. Also, I chose HDFS because it was the most convenient and what I was most comfortable with based on my experience so far in the class (working with it since week 2).</li>\n<li>Read the CSV files in via a Zeppelin notebook.  I chose to use Zeppelin because my goal was to both practice using Scala on Spark as well as to effectively communicate my steps in the project, including the option to be able to visualize the data</li>\n<li>Save the CSVs as parquets -> the files took a long time to load via CSV, but load times with parquet were much much faster (reduced from tens of minutes to less than 5 secs)</li>\n<li>See the rest of the notebook for the remaining steps.</li>\n<li>NOTE: <a href=\"https://data.bls.gov/cew/doc/layouts/csv_quarterly_layout.htm\">This file provides a relatively good explanation for the fields for the BLS data</a></li>\n</ol>\n<h3>Data Sources:</h3>\n<ol>\n<li><a href=\"https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/\">The storm events details files from the NOAA</a> provided data about stormevents since the 1950s through today in the US.</li>\n<li><a href=\"https://www.bls.gov/cew/datatoc.htm#NAICS_BASED\">BLS Quarterly census of employment and wages</a> (singlefile CSVs), provided a view of jobs by industry code in the US.</li>\n<li>To provide a human-readable view of the industry codes from the BLS, I also joined in their industry code file, found <a href=\"https://data.bls.gov/cew/doc/titles/industry/industry_titles.csv\">here</a>.  A description of this file can be found <a href=\"https://data.bls.gov/cew/doc/titles/industry/industry_titles.htm\">here</a></li>\n<li>This ended up not being necessary: In order to combine the two sources of data, I needed to incorporate a forth reference to &ldquo;translate&rdquo; between the code for location in the stormevents file and the code for area in the employment file.  I found this data <a href=\"https://www2.census.gov/geo/docs/reference/codes/files/national_county.txt\">also on the census site from the US government</a></li>\n</ol>\n<h3>Scripts not run from Zeppelin:</h3>\n<pre><code class=\"sh\">##Download all the stormevents data from the NOAA\n##Run on local windows machine\nwget --no-clobber --convert-links --random-wait -r -p -E -e robots=off -U mozilla https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/ \n</code></pre>\n<pre><code class=\"sh\">##Download all the jobs data from the BLS and unzip them\n##Run on local windows machine\nfor i in {1990..2018}; do echo \"https://data.bls.gov/cew/data/files/$i/csv/\"\"$i\"\"_qtrly_singlefile.zip\";\nwget \"https://data.bls.gov/cew/data/files/$i/csv/\"\"$i\"\"_qtrly_singlefile.zip\"; \nunzip \"$i\"\"_qtrly_singlefile.zip\"; rm \"$i\"\"_qtrly_singlefile.zip\"; done\n###Note: this I initially ran for the period from 1990 to 2018, but due to the size of the resulting files, I ended up only using data between 2011 and 2018\n####Though there was space on disk for most (if not all) of these files, the time it took to read in the data to the notebook was getting frustratingly long.\n</code></pre>\n<pre><code class=\"sh\">##This is the list of commands I ran per Jerry's instructions on how to mount the /data/ drive to HDFS\nmkdir -p /data/hadoop/hdfs/data  \nchown -R hdfs:hadoop /data/hadoop/hdfs/data #to match the ownership Hadoop expects\n##Next, I edited the HDFS configuration in Ambari to add, in a comma-separated list, the newly created directory to \"Datanode directories\" in Services / HDFS / Configs, alongside the original directory HDFS was using \n##Finally, I restarted HDFS from Ambari\n</code></pre>\n<pre><code class=\"sh\">##Copy the files from the VM to HDFS (done for both for the BLS data and the stormevents)\nhadoop fs -copyFromLocal /data/singlefiles /tmp/\n</code></pre>\n<p>If you are having trouble with the code, please provide a detailed description of what you are trying to do in order to demonstrate understanding of the course material</p>\n<h1>Presentation</h1>\n<p>Students will provide a brief (2-5 min) presentation of their project in the last class session to share with classmates.</p>\n<p>If you need to be absent please contact instructor so that appropriate arrangements can be made.</p>\n<p>Students can optionally record a video in advance of the last class and post to YouTube or similar for viewing, and then answer any questions live.</p>\n<p>Example Outline</p>\n<ol>\n<li>Introduce the data being used</li>\n<li>Describe the objective of the project</li>\n<li>Walk through approach, providing justifications of decisions made</li>\n<li>For example, don’t just say you stored your results in Redis, say why you stored your results in Redis</li>\n<li>Include code where appropriate</li>\n<li>Share results</li>\n<li>Issues you had, if any E.g. Something you would have wanted to do but couldn’t get working or what you would like to do with more time\n<br  />Ran into issue with data size, which seems counterintuitive.  Had to remove a number of files and only analyze 2010 through 2 quarters into 2018</li>\n<li>Answer any questions</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1551932449525_-219632287","id":"20190307-042049_634404678","dateCreated":"2019-03-07T04:20:49+0000","dateStarted":"2019-03-09T23:57:56+0000","dateFinished":"2019-03-09T23:57:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4180","title":"Summary"},{"text":"%sh\n###https://hortonworks.com/tutorial/getting-started-with-apache-zeppelin/#creating-an-interpreter \n####Verify that the files you need are in hdfs:\npwd\nls -l\nhadoop fs -ls -h /tmp/StormEvents #downloaded from https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/\nhadoop fs -ls -h /tmp/singlefiles #https://www.bls.gov/cew/datatoc.htm#NAICS_BASED (singlefile CSVs)\n##Disable run on this cell after initially confirming presence of files","user":"anonymous","dateUpdated":"2019-03-09T23:58:49+0000","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","editorHide":true,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1551040888933_1881781115","id":"20190224-204128_2017193334","dateCreated":"2019-02-24T20:41:28+0000","dateStarted":"2019-03-07T03:18:25+0000","dateFinished":"2019-03-07T03:18:33+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4181","title":"Add the shell interpreter"},{"text":"//Convert the csv files to parquet for easier future loading.  Disabled run in the notebook after initial save\nval stormevents_details = spark.read.option(\"header\", \"true\").csv(\"hdfs:///tmp/StormEvents/StormEvents_details-ftp_v1.0_*.csv.gz\")\n//Save as parquet\nstormevents_details.write.parquet(\"hdfs:///tmp/StormEvents/stormevents_details.parquet\")\nimport org.apache.spark.util.SizeEstimator\nprintln(SizeEstimator.estimate(stormevents_details))\n\n//read in the single csv files for employment stats\nval employment_stats = spark.read.option(\"header\", \"true\").csv(\"hdfs:////tmp/singlefiles/201*.q1-q*.singlefile.csv\")\n//save as parquet\nemployment_stats.write.parquet(\"hdfs:///tmp/singlefiles/employment_stats.parquet\")","user":"anonymous","dateUpdated":"2019-03-09T23:59:05+0000","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1552157114053_-867709063","id":"20190309-184514_1118030875","dateCreated":"2019-03-09T18:45:14+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:4182","title":"Convert the csv files to parquet"},{"text":"//Read in the stormevents parquet file and select only the columns necessary for the project\r\n//Additionally, create the \"Strm_FIPS_Code\" column so we can later join this data with the BLS data\r\n//Finally, create the \"qtr\" column to also aid in the join -> I'd like to be able to see the data broken out over time to see if I can answer whether events or jobs are increasing or decreasing over time\r\nval stormevents_details = spark.read.parquet(\"hdfs:///tmp/StormEvents/stormevents_details.parquet\").select(\"STATE\", \"STATE_FIPS\", \"BEGIN_YEARMONTH\",\"YEAR\",\"MONTH_NAME\",\"BEGIN_DAY\",\"EVENT_TYPE\", \"CZ_TYPE\",\"CZ_FIPS\", \"INJURIES_DIRECT\",\"INJURIES_INDIRECT\", \"DEATHS_DIRECT\", \"DEATHS_INDIRECT\").withColumn(\r\n    \"Strm_FIPS_Code\",concat($\"STATE_FIPS\", lit(\"0\"), $\"CZ_FIPS\")).withColumn(\"qtr\",quarter(concat($\"YEAR\", lit(\"-\"), substring($\"BEGIN_YEARMONTH\", -2, 1),  substring($\"BEGIN_YEARMONTH\", -1, 1),lit(\"-\"),$\"BEGIN_DAY\")))\r\n\r\n//Initially, I didn't realize that the county codes could be three digits, so my concat statement looked like: concat($\"STATE_FIPS\", lit(\"0\"), $\"CZ_FIPS\").  I had to update to something more sophisticated as shown above.\r\n//.withColumn(\"Strm_FIPS_Code\", length($\"CZ_FIPS\")) )\r\n\r\n//Preview the data\r\nstormevents_details.limit(5).show()\r\nprintln(stormevents_details.count())","user":"anonymous","dateUpdated":"2019-03-09T23:53:30+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1551038265212_-14820636","id":"20190224-195745_1693140395","dateCreated":"2019-02-24T19:57:45+0000","dateStarted":"2019-03-09T23:53:31+0000","dateFinished":"2019-03-09T21:24:07+0000","status":"RUNNING","progressUpdateIntervalMs":500,"$$hashKey":"object:4183","errorMessage":""},{"text":"//How is the data broken up by location?  Do the CZ_NAME data points make sense, or are they all over the place?  \nval wa_state = stormevents_details.filter($\"STATE\" === \"WASHINGTON\")\nwa_state.groupBy($\"STATE_FIPS\",$\"CZ_NAME\",$\"CZ_FIPS\").count.show(Int.MaxValue)","user":"anonymous","dateUpdated":"2019-03-09T23:59:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+--------------------+-------+-----+\n|STATE_FIPS|             CZ_NAME|CZ_FIPS|count|\n+----------+--------------------+-------+-----+\n|        53|Admiralty Inlet Area|    510|    1|\n|        53|       CENTRAL COAST|     16|   20|\n|        53|NE GRANT/X NE DOU...|     35|   11|\n|        53|THURSTON T X SE/S...|     11|    2|\n|        53|            GARFIELD|     23|   20|\n|        53|      X SW KLICKITAT|     24|   14|\n|        53|              SKAGIT|     57|   13|\n|        53|             LINCOLN|     43|   68|\n|        53|NORTHEAST BLUE MO...|     31|   41|\n|        53|                KING|     33|   77|\n|        53|SKAMANIA T X S & ...|     19|   13|\n|        53|    SIMCOE HIGHLANDS|    521|   27|\n|        53|             COWLITZ|     15|   46|\n|        53|  SOUTHWEST INTERIOR|     20|   85|\n|        53|S GARFIELD/SW ASOTIN|     31|   23|\n|        53|     MOSES LAKE AREA|     34|   95|\n|        53|         NORTH COAST|    516|   61|\n|        53|HOOD CANAL/KITSAP...|     10|   36|\n|        53|  SOUTHWEST INTERIOR|    504|   41|\n|        53|     HOOD CANAL AREA|    511|   44|\n|        53|C COWLITZ/N&NE CL...|     40|   10|\n|        53|        X S SKAMANIA|     23|    2|\n|        53|        Spokane Area|     36|    2|\n|        53|     Hood Canal Area|    511|    1|\n|        53|        PEND OREILLE|     51|   73|\n|        53|NORTHERN CASCADE ...|      3|   11|\n|        53|      VANCOUVER AREA|     39|   23|\n|        53|            FRANKLIN|     21|   40|\n|        53|              KITSAP|     35|   16|\n|        53|WEST SLOPES SOUTH...|     19|   20|\n|        53|WESTERN WHATCOM C...|    503|  122|\n|        53|CENTRAL COLUMBIA ...|     46|   23|\n|        53|          NE CLALLAM|     13|    9|\n|        53|     Okanogan Valley|     43|    9|\n|        53|EASTERN COLUMBIA ...|     24|   12|\n|        53|            OLYMPICS|    513|   35|\n|        53|      Western Skagit|    506|    1|\n|        53|NORTHWEST BLUE MO...|     30|   80|\n|        53|          E KITTITAS|     26|   25|\n|        53|            San Juan|      1|    1|\n|        53|            COLUMBIA|     13|   21|\n|        53| NORTHEAST MOUNTAINS|     37|  350|\n|        53|SOUTHERN WASHINGT...|     40|   36|\n|        53|East Slopes North...|     42|    8|\n|        53|     KITTITAS VALLEY|     26|   67|\n|        53|     OKANOGAN VALLEY|     43|  130|\n|        53| GRANT/LINCOLN/ADAMS|     33|    1|\n|        53| S GARFIELD/W ASOTIN|     31|    1|\n|        53| ISLAND/NE JEFFERSON|      9|   16|\n|        53|           JEFFERSON|     31|   14|\n|        53|N GARFIELD & NE A...|     32|    8|\n|        53|  WASHINGTON PALOUSE|     33|  128|\n|        53|  S GRANT & SW ADAMS|     34|    9|\n|        53|      W GRAYS HARBOR|     16|   13|\n|        53|  WATERVILLE PLATEAU|     44|   91|\n|        53|W CLALLAM/W JEFFE...|     15|    8|\n|        53|             DOUGLAS|     17|   34|\n|        53|            W PIERCE|      8|   15|\n|        53|BLUE MOUNTAIN FOO...|     29|   62|\n|        53|BENTON/FRANKLIN/W...|     28|   31|\n|        53|  W KING/X SE KITSAP|      7|   14|\n|        53|             WHATCOM|     73|   15|\n|        53|EAST YAKIMA/EAST ...|     27|    1|\n|        53|WESTERN COLUMBIA ...|     23|   23|\n|        53|Lower Chehalis Va...|    512|    1|\n|        53|  Washington Palouse|     33|    2|\n|        53|       CENTRAL COAST|    517|   64|\n|        53|CASCADES OF WHATC...|    567|   15|\n|        53|SOUTH WASHINGTON ...|     19|   89|\n|        53|Northeast Blue Mo...|     31|    1|\n|        53|         SOUTH COAST|     21|  181|\n|        53|EAST PUGET SOUND ...|    505|   48|\n|        53|         Tacoma Area|    509|    1|\n|        53|GREATER VANCOUVER...|     39|   35|\n|        53|        GRAYS HARBOR|     27|   24|\n|        53|MASON T SW & NW/K...|     10|   18|\n|        53|              ASOTIN|      3|   37|\n|        53|EAST SLOPES OF TH...|    520|   57|\n|        53|          NC CLALLAM|     14|    3|\n|        53| NORTHEAST MOUNTAINS|     35|    1|\n|        53|NC&S OKANOGAN X E...|     43|   17|\n|        53|LOWER COLUMBIA BASIN|     28|  136|\n|        53|EVERETT AND VICINITY|    507|   42|\n|        53|             WHITMAN|     75|   80|\n|        53|              WAZ001|      0|    1|\n|        53|            KITTITAS|     37|   38|\n|        53|            SAN JUAN|      1|   98|\n|        53|WEST SLOPES CENTR...|    519|  102|\n|        53|           SNOHOMISH|     61|   51|\n|        53|            THURSTON|     67|   23|\n|        53|       Central Coast|    517|    1|\n|        53|SOUTHERN CASCADE ...|     40|   11|\n|        53|E CHELAN/EXTREME ...|     41|   12|\n|        53|              BENTON|      5|   80|\n|        53|ADMIRALTY INLET AREA|      9|   27|\n|        53|WESTERN STRAIT OF...|    515|   20|\n|        53|              ISLAND|     29|    6|\n|        53|            OLYMPICS|     12|   10|\n|        53|            SW CLARK|     39|   23|\n|        53|BENTON/FRANKLIN/W...|     28|    2|\n|        53|EASTERN STRAIT OF...|    514|   24|\n|        53|           W PACIFIC|     21|   44|\n|        53|WESTERN SKAGIT CO...|    506|   36|\n|        53|NORTHERN CASADE F...|      3|    9|\n|        53|W KITTITAS/W YAKI...|     25|   29|\n|        53|C SKAGIT/C SNOHOMISH|      3|    9|\n|        53|               MASON|     45|   13|\n|        53|SEATTLE METROPOLI...|      7|   34|\n|        53|      LOWER COLUMBIA|     22|   40|\n|        53|     Western Whatcom|    503|    1|\n|        53|C KING/C PIERCE/X...|      4|   15|\n|        53|WESTERN COLUMBIA ...|     45|   24|\n|        53|              CHELAN|      7|  125|\n|        53|         North Coast|    516|    1|\n|        53|           KLICKITAT|     39|   18|\n|        53|WEST SLOPES CENTR...|     18|    5|\n|        53|              YAKIMA|     77|  112|\n|        53|EAST SLOPES NORTH...|     42|  314|\n|        53|   DOUGLAS/SE CHELAN|     37|    5|\n|        53|ADMIRALTY INLET AREA|    510|   86|\n|        53|        SW SNOHOMISH|      6|   16|\n|        53|                null|      0|    1|\n|        53|EAST COLUMBIA RIV...|     24|   29|\n|        53|UPPER COLUMBIA BASIN|     35|   92|\n|        53|      Vancouver Area|     39|    1|\n|        53|               ADAMS|      1|   32|\n|        53|            SKAMANIA|     59|    8|\n|        53|CENTRAL CASCADE F...|      4|   13|\n|        53|E PACIFIC/W LEWIS...|     20|    5|\n|        53|         TACOMA AREA|      8|   25|\n|        53|SEATTLE/BREMERTON...|    508|   36|\n|        53|               FERRY|     19|   69|\n|        53|       C&SE OKANOGAN|     36|    6|\n|        53|LOWER GARFIELD & ...|     32|   31|\n|        53|  SOUTHWEST INTERIOR|     11|   29|\n|        53|EAST SLOPE OF THE...|     25|    3|\n|        53|            OKANOGAN|     47|  194|\n|        53|X SE WALLA WALLA/...|     30|   21|\n|        53|EASTERN STRAIT OF...|     13|   18|\n|        53|CASCADES OF SNOHO...|    568|   13|\n|        53|LOWER CHEHALIS VA...|    512|    5|\n|        53|            E YAKIMA|     27|   19|\n|        53|WEST SLOPES NORTH...|     17|    6|\n|        53|             PACIFIC|     49|   21|\n|        53|EAST PUGET SOUND ...|    555|   14|\n|        53|WEST COLUMBIA RIV...|     23|   15|\n|        53|   NE OKANOGAN/FERRY|     38|   28|\n|        53|               CLARK|     11|   55|\n|        53|SEATTLE AND VICINITY|    558|   10|\n|        53|NE LINCOLN/SPOKAN...|     36|   20|\n|        53|DOUGLAS X EXTREME...|     44|   14|\n|        53|East Slopes Of Th...|    520|    2|\n|        53|Northwest Blue Mo...|     30|    2|\n|        53|East Columbia Riv...|     24|    1|\n|        53|STEVENS/PEND OREI...|     37|   48|\n|        53|E KING/E PIERCE/E...|     18|   12|\n|        53|W SKAGIT/NW SNOHO...|      5|   16|\n|        53|BELLEVUE AND VICI...|    556|    5|\n|        53|WEST SLOPES NORTH...|    518|   91|\n|        53|Upper Columbia Basin|     35|    5|\n|        53|CASCADES OF PIERC...|    569|    9|\n|        53|               GRANT|     25|   76|\n|        53|  NORTHWEST INTERIOR|      5|   33|\n|        53|THURSTON T X SE/S...|     11|   19|\n|        53|C LEWIS/CLARK T S...|     40|    1|\n|        53|CENTRAL SLOPES NO...|     18|    2|\n|        53| Northeast Mountains|     37|   11|\n|        53|EVERETT AND VICINITY|      6|   37|\n|        53|             STEVENS|     65|   85|\n|        53|             SPOKANE|     63|  183|\n|        53|      Wenatchee Area|     41|    3|\n|        53|N COLUMBIA/SE WAL...|     29|   23|\n|        53|     Moses Lake Area|     34|    6|\n|        53|S CLALLAM T X SW/...|     12|    3|\n|        53|            SAN JUAN|     55|    4|\n|        53|W OKANOGAN/C&W CH...|     42|   37|\n|        53|W COWLITZ/WIAKIAK...|     22|    8|\n|        53|  Okanogan Highlands|     38|    7|\n|        53|             CLALLAM|      9|   14|\n|        53|        SPOKANE AREA|     36|  217|\n|        53|Lower Garfield & ...|     32|    3|\n|        53|X S SKAMANIA/X SE...|     23|    9|\n|        53|         NORTH COAST|     15|   20|\n|        53|           WAHKIAKUM|     69|   36|\n|        53|EAST SLOPES OF TH...|    502|   33|\n|        53|EAST SLOPES SOUTH...|     25|   42|\n|        53|WESTERN STRAIT OF...|     14|    7|\n|        53|      WENATCHEE AREA|     41|  129|\n|        53|CENTRAL CASADE FO...|      4|   12|\n|        53|C&E WHATCOM/E SKA...|     17|    6|\n|        53|Blue Mountain Foo...|     29|    2|\n|        53|WHITMAN & EXTREME...|     33|   18|\n|        53|       YAKIMA VALLEY|     27|   97|\n|        53|         WALLA WALLA|     71|   80|\n|        53|  Waterville Plateau|     44|    7|\n|        53|NE OKANOGAN/FERRY...|     35|    6|\n|        53|  OKANOGAN HIGHLANDS|     38|  153|\n|        53|              PIERCE|     53|   34|\n|        53|Everett and Vicinity|    507|    1|\n|        53|BREMERTON AND VIC...|    559|    7|\n|        53|               LEWIS|     41|   16|\n|        53|EAST SLOPES OF TH...|    501|   32|\n|        53|         TACOMA AREA|    509|   29|\n|        53|DOUGLAS/LOWER CHELAN|     37|    1|\n|        53|        WAZ001 - 003|      0|    1|\n|        53|     WESTERN WHATCOM|      2|   55|\n+----------+--------------------+-------+-----+\n\nwa_state: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [BEGIN_YEARMONTH: string, BEGIN_DAY: string ... 49 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552158162865_-157431737","id":"20190309-190242_1808136587","dateCreated":"2019-03-09T19:02:42+0000","dateStarted":"2019-03-09T20:52:45+0000","dateFinished":"2019-03-09T20:52:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4184","title":"Explore the Storm Data"},{"text":"//Read in the employment stats parquet file create earlier and select only a subset of columns relevant to my objective\nval employment_stats = spark.read.parquet(\"hdfs:///tmp/singlefiles/employment_stats.parquet\").select(\"area_fips\",\"own_code\",\"industry_code\",\"agglvl_code\",\"size_code\",\"year\",\"qtr\",\"disclosure_code\",\"qtrly_estabs\",\"month1_emplvl\",\"month2_emplvl\",\"month3_emplvl\")\n\n//Preview the data\nemployment_stats.limit(5).show()\nprintln(employment_stats.count())","user":"anonymous","dateUpdated":"2019-03-10T00:00:07+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job 103 cancelled part of cancelled job group zeppelin-2E4BKYXQP-20190305-031318_593540716\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1609)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1544)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:811)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1799)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1779)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1768)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:611)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\n  at scala.Option.orElse(Option.scala:289)\n  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:201)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:392)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\n  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:606)\n  ... 47 elided\n"}]},"apps":[],"jobName":"paragraph_1551755598329_-356647352","id":"20190305-031318_593540716","dateCreated":"2019-03-05T03:13:18+0000","dateStarted":"2019-03-09T23:39:10+0000","dateFinished":"2019-03-09T23:40:08+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:4185","title":"Read in the BLS Employment Census Data"},{"text":"//Read in the industry code data\nval industry_codes = spark.read.option(\"header\", \"true\").csv(\"hdfs:///tmp/industry_titles.csv\")\n//Combine this with the employment_stats dataframeval columnsToSum = List(col(\"month1_emplvl\"), col(\"month2_emplvl\"), col(\"month1_emplvl\")) //List the columns to sum \nval employment_stats_with_indus = employment_stats.join(industry_codes, \"industry_code\").withColumn(\"Total_EmpLVL\",columnsToSum.reduce(_ + _))\nemployment_stats_with_indus.limit(5).show()","user":"anonymous","dateUpdated":"2019-03-10T00:00:25+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job 102 cancelled part of cancelled job group zeppelin-2E4BKYXQP-20190309-212938_1166744429\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1609)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1544)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:811)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1799)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1779)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1768)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n  at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:148)\n  at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\n  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\n  at scala.Option.orElse(Option.scala:289)\n  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:201)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:392)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:596)\n  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:473)\n  ... 47 elided\n"}]},"apps":[],"jobName":"paragraph_1552166978816_502196849","id":"20190309-212938_1166744429","dateCreated":"2019-03-09T21:29:38+0000","dateStarted":"2019-03-09T23:31:48+0000","dateFinished":"2019-03-09T23:39:07+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:4186","title":"Read and Join in the Industry Code Data"},{"text":"//Combine the two data sources (we only have employment data by country from 1990 to 2018, but natural disaster data from 1950 onwards; thus we combine on the employment data and drop all the natural disaster data earlier than 1990\n\nval storms_EmpbyCounty = stormevents_details.join(employment_stats_with_indus, $\"Strm_FIPS_Code\" === $\"area_fips\")\nstorms_EmpbyCounty.limit(5).show()\nprintln(storms_EmpbyCounty.count())","user":"anonymous","dateUpdated":"2019-03-10T00:00:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1551576693396_1718812877","id":"20190303-013133_926423602","dateCreated":"2019-03-03T01:31:33+0000","dateStarted":"2019-03-09T21:43:17+0000","dateFinished":"2019-03-09T22:09:19+0000","status":"ABORT","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4187","title":"Join the Storm and Employment Data"},{"text":"//Combining the two data sources directly took forever:\n//Instead, first filter the data app\n\n//Create a column summarizing the per month employment levels as well so it matchs up better with the Storm Events data\nval columnsToSum = List(col(\"month1_emplvl\"), col(\"month2_emplvl\"), col(\"month1_emplvl\")) //List the columns to sum \n.withColumn(\"Total_EmpLVL\",columnsToSum.reduce(_ + _))","user":"anonymous","dateUpdated":"2019-03-09T23:31:47+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1551928287653_1526221155","id":"20190307-031127_1080524228","dateCreated":"2019-03-07T03:11:27+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:4188"}],"name":"FinalBD210/Final Project","id":"2E4BKYXQP","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}
