{"paragraphs":[{"title":"Summary","text":"%md\n#README\n\n###Objective:\nMy goal was to pull in weather and storm events over time in order to gain an understanding of which places in the US have the least risk of dangerous storms occuring or which generally have bad weather (based on historical data in both cases), and once those locations are determined, figure out which industries are located in those same places in order to determine roughly what types of jobs might be available in the safer and more pleasant climates to live in. \n\n###Approach:\n1. I chose to use Zeppelin because my goal was to both practice using Scala on Spark as well as to effectively communicate my steps in the project, including the option to be able to visualize the data.\n    I chose to use Scala on Spark because, based on what we learned in the class, Spark is written in Scala and may be faster.  Also, part of my goal for this class was specifically to learn Scala (as I already know Python and R relatively well).\n2. See sources of data listed below.\n3. Each of the sources were csvs (through the storm-events data was gzip'd) or txt files.  I wrote a simple shell script to download the files (there were many, broken out by year, so I didn't want to download each by hand) to my local machine.  See the section \"Scripts not run from Zeppelin\" below.\n    _Issue:_ Initially, I wanted to use the XLS files available on the BLS site -> they were laid out in a much more human-readable fashion and I wouldn't have to combine in the industry code file.  I did find that I could load in individual excel files if I used the library \"com.crealytics:spark-excel_2.11:0.11.1\", but I could not figure out an easy way to read in all the files in Scala in Zeppelin.  Reading in multiple CSVs is easy with the wildcard (*), so I ended up re-working the project with those.  [Here](https://github.com/crealytics/spark-excel) is the link to the repo for that library which I was using as a reference.\n    _Question:_ I wasn't sure which approach was better (I wasn't able to successfully test it) -> Should I unzip the files prior to adding them into HDFS, or should I unzip them as I read them into memory? \n    _Issue:_ Ran out of space in HDFS-> Per some instructions laid out by Jerry, mount the data directory onto HDFS (steps written out below)\n4. Transferred the files to the /data/ directory on the VM using WINSCP and loaded them onto HDFS\n    I chose HDFS because it seemed to fit the data I was uploading -> normalized, column-based data whose size fit the criteria (not too small, especially once combined). I wasn't intending to do anything too interactive with the data either. Also, I chose HDFS because it was the most convenient and what I was most comfortable with based on my experience so far in the class (working with it since week 2).\n5. Read the CSV files in via a Zeppelin notebook, then combine them with other smaller data sets.\n6. Save the CSVs as parquets -> the files took a long time to load via CSV, but load times with parquet were much much faster (reduced from tens of minutes to less than 5 secs)\n7. I saved the dataframes again as parquet after more processing and analyzing so I could break up break the notebook up into two separate ones.  This seemed to help with memory issues.\n   Additionally, I used persist and unpersist a bit -> this seemed to help, but did not wholly prevent me from running into memory issues.\n8. See the rest of the notebook for the remaining steps/code and results\n\n\n###Data Sources:\n1. Size: 1.2G (post gunzip) | [The storm events details files from the NOAA](https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/) provided data about stormevents since the 1950s through today in the US.  This data totalled about \n2. Size: 17.5G | [BLS Quarterly census of employment and wages](https://www.bls.gov/cew/datatoc.htm#NAICS_BASED) (singlefile CSVs), provided a view of jobs by industry code in the US.  \n3. Size: 139K | To provide a human-readable view of the industry codes from the BLS, I also joined in their industry code file, found [here](https://data.bls.gov/cew/doc/titles/industry/industry_titles.csv).  A description of this file can be found [here](https://data.bls.gov/cew/doc/titles/industry/industry_titles.htm)\n4. Size: 93K | In order to get clean location names, I needed to incorporate a forth reference to \"translate\" the FIPs code (the weather data location names were not consistent).  I found this data [also on the census site from the US government](https://www2.census.gov/geo/docs/reference/codes/files/national_county.txt)\n5. Size: 2.3K | [Acreage by State](https://www.fs.usda.gov/Internet/FSE_DOCUMENTS/fsm8_037652.htm)\n\n\n###Notes:\n* [This file provides a relatively good explanation for the fields for the BLS data](https://data.bls.gov/cew/doc/layouts/csv_quarterly_layout.htm)\n* _Issue:_ I ran into a problem with YARN where paragraphs would get stuck at \"RUNNING: 0%\" for much longer than expected.  Prior to the issue, paragraphs would take <20 seconds.  After the issue, I had to kill the jobs after 30 minutes.  Switching to local mode (master = local[*]) in the interpreter fixed this issue.  \n* _Issue:_ I ran into an issue where I received the error \"java.lang.OutOfMemoryError: Java heap space\" quite a bit.  There were workarounds mentioned that helped me get through some steps, but I spent most of my time on this project trying to think of ways to workaround this.   * One approach, for example, was separating out the notebooks and saving at particular points where I though a transition was logical to parquet again.  Another approach was just reducing the volume of data used, and only using a subset of data to move onto future steps (for    example, just using the top ten states by event count per acre as the \"base\" data to use for identifying counties to investigate for jobs)\n  * Originally, when I had vegas added to the notebook, I'd run into issues early on: removing it allowed me to move forward for the most part.\n* See the \"Future Improvements\" paragraph, if there is time, after the going through the script for improvements.\n\n\n###Scripts not run from Zeppelin:\n```sh\n##Download all the stormevents data from the NOAA\n##Run on local windows machine\nwget --no-clobber --convert-links --random-wait -r -p -E -e robots=off -U mozilla https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/ \n```\n```sh\n##Download all the jobs data from the BLS and unzip them\n##Run on local windows machine\nfor i in {1990..2018}; do echo \"https://data.bls.gov/cew/data/files/$i/csv/\"\"$i\"\"_qtrly_singlefile.zip\";\nwget \"https://data.bls.gov/cew/data/files/$i/csv/\"\"$i\"\"_qtrly_singlefile.zip\"; \nunzip \"$i\"\"_qtrly_singlefile.zip\"; rm \"$i\"\"_qtrly_singlefile.zip\"; done\n###Note: this I initially ran for the period from 1990 to 2018, but due to the size of the resulting files, I ended up only using data between 2011 and 2018\n####Though there was space on disk for most (if not all) of these files, the time it took to read in the data to the notebook was getting frustratingly long.\n```\n```sh\n##This is the list of commands I ran per Jerry's instructions on how to mount the /data/ drive to HDFS\nmkdir -p /data/hadoop/hdfs/data  \nchown -R hdfs:hadoop /data/hadoop/hdfs/data #to match the ownership Hadoop expects\n##Next, I edited the HDFS configuration in Ambari to add, in a comma-separated list, the newly created directory to \"Datanode directories\" in Services / HDFS / Configs, alongside the original directory HDFS was using \n##Finally, I restarted HDFS from Ambari\n```\n```sh\n##Copy the files from the VM to HDFS (done for both for the BLS data and the stormevents)\nhadoop fs -copyFromLocal /data/singlefiles /tmp/\n```\n","user":"anonymous","dateUpdated":"2019-03-12T04:31:50+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>README</h1>\n<h3>Objective:</h3>\n<p>My goal was to pull in weather and storm events over time in order to gain an understanding of which places in the US have the least risk of dangerous storms occuring or which generally have bad weather (based on historical data in both cases), and once those locations are determined, figure out which industries are located in those same places in order to determine roughly what types of jobs might be available in the safer and more pleasant climates to live in.</p>\n<h3>Approach:</h3>\n<ol>\n<li>I chose to use Zeppelin because my goal was to both practice using Scala on Spark as well as to effectively communicate my steps in the project, including the option to be able to visualize the data.\n<br  />I chose to use Scala on Spark because, based on what we learned in the class, Spark is written in Scala and may be faster.  Also, part of my goal for this class was specifically to learn Scala (as I already know Python and R relatively well).</li>\n<li>See sources of data listed below.</li>\n<li>Each of the sources were csvs (through the storm-events data was gzip'd) or txt files.  I wrote a simple shell script to download the files (there were many, broken out by year, so I didn't want to download each by hand) to my local machine.  See the section &ldquo;Scripts not run from Zeppelin&rdquo; below.\n<br  /><em>Issue:</em> Initially, I wanted to use the XLS files available on the BLS site -> they were laid out in a much more human-readable fashion and I wouldn't have to combine in the industry code file.  I did find that I could load in individual excel files if I used the library &ldquo;com.crealytics:spark-excel_2.11:0.11.1&rdquo;, but I could not figure out an easy way to read in all the files in Scala in Zeppelin.  Reading in multiple CSVs is easy with the wildcard (*), so I ended up re-working the project with those.  <a href=\"https://github.com/crealytics/spark-excel\">Here</a> is the link to the repo for that library which I was using as a reference.\n<br  /><em>Question:</em> I wasn't sure which approach was better (I wasn't able to successfully test it) -> Should I unzip the files prior to adding them into HDFS, or should I unzip them as I read them into memory?\n<br  /><em>Issue:</em> Ran out of space in HDFS-> Per some instructions laid out by Jerry, mount the data directory onto HDFS (steps written out below)</li>\n<li>Transferred the files to the /data/ directory on the VM using WINSCP and loaded them onto HDFS\n<br  />I chose HDFS because it seemed to fit the data I was uploading -> normalized, column-based data whose size fit the criteria (not too small, especially once combined). I wasn't intending to do anything too interactive with the data either. Also, I chose HDFS because it was the most convenient and what I was most comfortable with based on my experience so far in the class (working with it since week 2).</li>\n<li>Read the CSV files in via a Zeppelin notebook, then combine them with other smaller data sets.</li>\n<li>Save the CSVs as parquets -> the files took a long time to load via CSV, but load times with parquet were much much faster (reduced from tens of minutes to less than 5 secs)</li>\n<li>I saved the dataframes again as parquet after more processing and analyzing so I could break up break the notebook up into two separate ones.  This seemed to help with memory issues.\n<br  />Additionally, I used persist and unpersist a bit -> this seemed to help, but did not wholly prevent me from running into memory issues.</li>\n<li>See the rest of the notebook for the remaining steps/code and results</li>\n</ol>\n<h3>Data Sources:</h3>\n<ol>\n<li>Size: 1.2G (post gunzip) | <a href=\"https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/\">The storm events details files from the NOAA</a> provided data about stormevents since the 1950s through today in the US.  This data totalled about</li>\n<li>Size: 17.5G | <a href=\"https://www.bls.gov/cew/datatoc.htm#NAICS_BASED\">BLS Quarterly census of employment and wages</a> (singlefile CSVs), provided a view of jobs by industry code in the US.</li>\n<li>Size: 139K | To provide a human-readable view of the industry codes from the BLS, I also joined in their industry code file, found <a href=\"https://data.bls.gov/cew/doc/titles/industry/industry_titles.csv\">here</a>.  A description of this file can be found <a href=\"https://data.bls.gov/cew/doc/titles/industry/industry_titles.htm\">here</a></li>\n<li>Size: 93K | In order to get clean location names, I needed to incorporate a forth reference to &ldquo;translate&rdquo; the FIPs code (the weather data location names were not consistent).  I found this data <a href=\"https://www2.census.gov/geo/docs/reference/codes/files/national_county.txt\">also on the census site from the US government</a></li>\n<li>Size: 2.3K | <a href=\"https://www.fs.usda.gov/Internet/FSE_DOCUMENTS/fsm8_037652.htm\">Acreage by State</a></li>\n</ol>\n<h3>Notes:</h3>\n<ul>\n<li><a href=\"https://data.bls.gov/cew/doc/layouts/csv_quarterly_layout.htm\">This file provides a relatively good explanation for the fields for the BLS data</a></li>\n<li><em>Issue:</em> I ran into a problem with YARN where paragraphs would get stuck at &ldquo;RUNNING: 0%&rdquo; for much longer than expected.  Prior to the issue, paragraphs would take &lt;20 seconds.  After the issue, I had to kill the jobs after 30 minutes.  Switching to local mode (master = local[*]) in the interpreter fixed this issue.</li>\n<li><em>Issue:</em> I ran into an issue where I received the error &ldquo;java.lang.OutOfMemoryError: Java heap space&rdquo; quite a bit.  There were workarounds mentioned that helped me get through some steps, but I spent most of my time on this project trying to think of ways to workaround this.   * One approach, for example, was separating out the notebooks and saving at particular points where I though a transition was logical to parquet again.  Another approach was just reducing the volume of data used, and only using a subset of data to move onto future steps (for    example, just using the top ten states by event count per acre as the &ldquo;base&rdquo; data to use for identifying counties to investigate for jobs)</li>\n<li>Originally, when I had vegas added to the notebook, I'd run into issues early on: removing it allowed me to move forward for the most part.</li>\n<li>See the &ldquo;Future Improvements&rdquo; paragraph, if there is time, after the going through the script for improvements.</li>\n</ul>\n<h3>Scripts not run from Zeppelin:</h3>\n<pre><code class=\"sh\">##Download all the stormevents data from the NOAA\n##Run on local windows machine\nwget --no-clobber --convert-links --random-wait -r -p -E -e robots=off -U mozilla https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/ \n</code></pre>\n<pre><code class=\"sh\">##Download all the jobs data from the BLS and unzip them\n##Run on local windows machine\nfor i in {1990..2018}; do echo \"https://data.bls.gov/cew/data/files/$i/csv/\"\"$i\"\"_qtrly_singlefile.zip\";\nwget \"https://data.bls.gov/cew/data/files/$i/csv/\"\"$i\"\"_qtrly_singlefile.zip\"; \nunzip \"$i\"\"_qtrly_singlefile.zip\"; rm \"$i\"\"_qtrly_singlefile.zip\"; done\n###Note: this I initially ran for the period from 1990 to 2018, but due to the size of the resulting files, I ended up only using data between 2011 and 2018\n####Though there was space on disk for most (if not all) of these files, the time it took to read in the data to the notebook was getting frustratingly long.\n</code></pre>\n<pre><code class=\"sh\">##This is the list of commands I ran per Jerry's instructions on how to mount the /data/ drive to HDFS\nmkdir -p /data/hadoop/hdfs/data  \nchown -R hdfs:hadoop /data/hadoop/hdfs/data #to match the ownership Hadoop expects\n##Next, I edited the HDFS configuration in Ambari to add, in a comma-separated list, the newly created directory to \"Datanode directories\" in Services / HDFS / Configs, alongside the original directory HDFS was using \n##Finally, I restarted HDFS from Ambari\n</code></pre>\n<pre><code class=\"sh\">##Copy the files from the VM to HDFS (done for both for the BLS data and the stormevents)\nhadoop fs -copyFromLocal /data/singlefiles /tmp/\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1551932449525_-219632287","id":"20190307-042049_634404678","dateCreated":"2019-03-07T04:20:49+0000","dateStarted":"2019-03-12T04:31:50+0000","dateFinished":"2019-03-12T04:31:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6157"},{"title":"Add the shell interpreter","text":"%sh\n###https://hortonworks.com/tutorial/getting-started-with-apache-zeppelin/#creating-an-interpreter \n####Verify that the files you need are in hdfs:\npwd\n#hadoop fs -rm -R /tmp/StormEvents/stormeventsDetailsWithNames.parquet\nhadoop fs -rm -R hdfs:///tmp/singlefiles/employment_stats_with_indus.parquet\n#ls -l\n#hadoop fs -ls -h /tmp/StormEvents #downloaded from https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/\n#hadoop fs -ls -h /tmp/singlefiles #https://www.bls.gov/cew/datatoc.htm#NAICS_BASED (singlefile CSVs)\n\n##Disable run on this cell after initially confirming presence of files","user":"anonymous","dateUpdated":"2019-03-12T01:40:51+0000","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","editorHide":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"/home/zeppelin\n19/03/10 18:48:34 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox-hdp.hortonworks.com:8020/tmp/singlefiles/employment_stats_with_indus.parquet' to trash at: hdfs://sandbox-hdp.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/singlefiles/employment_stats_with_indus.parquet1552243714917\n"}]},"apps":[],"jobName":"paragraph_1551040888933_1881781115","id":"20190224-204128_2017193334","dateCreated":"2019-02-24T20:41:28+0000","dateStarted":"2019-03-10T18:48:27+0000","dateFinished":"2019-03-10T18:48:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6158"},{"title":"Convert the csv files to parquet","text":"//Convert the csv files to parquet for easier future loading.  Disabled run in the notebook after initial save\nval stormevents_details = spark.read.option(\"header\", \"true\").csv(\"hdfs:///tmp/StormEvents/StormEvents_details-ftp_v1.0_*.csv.gz\")\n//Save as parquet\nstormevents_details.write.parquet(\"hdfs:///tmp/StormEvents/stormevents_details.parquet\")\nimport org.apache.spark.util.SizeEstimator\nprintln(SizeEstimator.estimate(stormevents_details))\n\n//read in the single csv files for employment stats\nval employment_stats = spark.read.option(\"header\", \"true\").csv(\"hdfs:////tmp/singlefiles/201*.q1-q*.singlefile.csv\")\n//save as parquet\nemployment_stats.write.parquet(\"hdfs:///tmp/singlefiles/employment_stats.parquet\")","user":"anonymous","dateUpdated":"2019-03-12T01:51:05+0000","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1552157114053_-867709063","id":"20190309-184514_1118030875","dateCreated":"2019-03-09T18:45:14+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:6159"},{"title":"Read in the Storm Data","text":"//Read in the stormevents parquet file and select only the columns necessary for the project\r\n//Additionally, create the \"Strm_FIPS_Code\" column so we can later join this data with the BLS data\r\n//Finally, create the \"qtr\" column to also aid in the join -> I'd like to be able to see the data broken out over time to see if I can answer whether events or jobs are increasing or decreasing over time\r\nval stormevents_details = spark.read.parquet(\"hdfs:///tmp/StormEvents/stormevents_details.parquet\").select(\"STATE\", \"STATE_FIPS\", \"BEGIN_YEARMONTH\",\"YEAR\",\"MONTH_NAME\",\"BEGIN_DAY\",\"EVENT_TYPE\", \"CZ_TYPE\",\"CZ_FIPS\", \"INJURIES_DIRECT\",\"INJURIES_INDIRECT\", \"DEATHS_DIRECT\", \"DEATHS_INDIRECT\").withColumn(\"Strm_FIPS_Code\",when(length($\"CZ_FIPS\") === 2, concat($\"STATE_FIPS\", lit(\"0\"), $\"CZ_FIPS\")).when(length($\"CZ_FIPS\") === 3, concat($\"STATE_FIPS\", $\"CZ_FIPS\")).otherwise(when(length($\"CZ_FIPS\") === 1, concat($\"STATE_FIPS\", lit(\"00\"), $\"CZ_FIPS\")))).withColumn(\"qtr\",quarter(concat($\"YEAR\", lit(\"-\"), substring($\"BEGIN_YEARMONTH\", -2, 1),  substring($\"BEGIN_YEARMONTH\", -1, 1),lit(\"-\"),$\"BEGIN_DAY\")))\r\n//Initially, I didn't realize that the county codes could be three digits, so my concat statement looked like: concat($\"STATE_FIPS\", lit(\"0\"), $\"CZ_FIPS\").  I had to update to something more sophisticated as shown above.\r\n\r\n\r\n//Preview the data\r\nstormevents_details.limit(5).show()\r\nprintln(stormevents_details.count())","user":"anonymous","dateUpdated":"2019-03-12T02:46:01+0000","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"stormevents_details: org.apache.spark.sql.DataFrame = [STATE: string, STATE_FIPS: string ... 13 more fields]\n"}]},"apps":[],"jobName":"paragraph_1551038265212_-14820636","id":"20190224-195745_1693140395","dateCreated":"2019-02-24T19:57:45+0000","dateStarted":"2019-03-10T18:09:01+0000","dateFinished":"2019-03-10T18:09:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6160"},{"title":"Read in, merge and save the County Names Data (save to Parquet)","text":"//Read in the County Names Data and join this data with stormevents_details\r\nval columns=Array(\"STATE_SHORT\", \"STATE_FIPS\", \"CZ_FIPS_LONG\",\"CZ_NAME\",\"CZ_TYPE2\")\r\nval area_names = spark.read.option(\"header\", \"false\").csv(\"file:///data/national_county.txt\").toDF(columns: _*).withColumn(\"Strm_FIPS_Code\",when(length($\"CZ_FIPS_LONG\") === 2, concat($\"STATE_FIPS\", lit(\"0\"), $\"CZ_FIPS_LONG\")).when(length($\"CZ_FIPS_LONG\") === 3, concat($\"STATE_FIPS\", $\"CZ_FIPS_LONG\")).otherwise(when(length($\"CZ_FIPS_LONG\") === 1, concat($\"STATE_FIPS\", lit(\"00\"), $\"CZ_FIPS_LONG\"))))\r\n\r\n//val stormeventsDetailsWithNames = stormevents_details.join(area_names, stormevents_details(\"Strm_FIPS_Code\") === area_names(\"Strm_FIPS_Code\") && stormevents_details(\"STATE_FIPS\") === area_names(\"STATE_FIPS\"))\r\nval stormeventsDetailsWithNames = stormevents_details.join(area_names, Seq(\"Strm_FIPS_Code\",\"STATE_FIPS\"))\r\n\r\nstormeventsDetailsWithNames.write.parquet(\"hdfs:///tmp/StormEvents/stormeventsDetailsWithNames.parquet\")\r\n//stormeventsDetailsWithNames.limit(5).show()","user":"anonymous","dateUpdated":"2019-03-12T02:46:20+0000","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"columns: Array[String] = Array(STATE_SHORT, STATE_FIPS, CZ_FIPS_LONG, CZ_NAME, CZ_TYPE2)\narea_names: org.apache.spark.sql.DataFrame = [STATE_SHORT: string, STATE_FIPS: string ... 4 more fields]\nstormeventsDetailsWithNames: org.apache.spark.sql.DataFrame = [Strm_FIPS_Code: string, STATE_FIPS: string ... 17 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552179419349_1383578652","id":"20190310-005659_2021319302","dateCreated":"2019-03-10T00:56:59+0000","dateStarted":"2019-03-10T18:16:08+0000","dateFinished":"2019-03-10T18:19:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6162"},{"title":"Read in the Final Storm Events Parquet","text":"//Read in the saved Parquet file from HDFS\nval stormeventsDetailsWithNames = spark.read.parquet(\"hdfs:///tmp/StormEvents/stormeventsDetailsWithNames.parquet\")\nstormeventsDetailsWithNames.persist()\nstormeventsDetailsWithNames.limit(5).show()","user":"anonymous","dateUpdated":"2019-03-12T04:23:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+----------+---------+---------------+----+----------+---------+----------+-------+-------+---------------+-----------------+-------------+---------------+---+-----------+------------+-----------------+--------+\n|Strm_FIPS_Code|STATE_FIPS|    STATE|BEGIN_YEARMONTH|YEAR|MONTH_NAME|BEGIN_DAY|EVENT_TYPE|CZ_TYPE|CZ_FIPS|INJURIES_DIRECT|INJURIES_INDIRECT|DEATHS_DIRECT|DEATHS_INDIRECT|qtr|STATE_SHORT|CZ_FIPS_LONG|          CZ_NAME|CZ_TYPE2|\n+--------------+----------+---------+---------------+----+----------+---------+----------+-------+-------+---------------+-----------------+-------------+---------------+---+-----------+------------+-----------------+--------+\n|         27017|        27|MINNESOTA|         200912|2009|  December|       23|Heavy Snow|      Z|     17|              0|                0|            0|              0|  4|         MN|         017|   Carlton County|      H1|\n|         27027|        27|MINNESOTA|         200912|2009|  December|       23|Heavy Snow|      Z|     27|              0|                0|            0|              0|  4|         MN|         027|      Clay County|      H1|\n|         27009|        27|MINNESOTA|         200912|2009|  December|       23|Heavy Snow|      Z|      9|              0|                0|            0|              0|  4|         MN|         009|    Benton County|      H1|\n|         55003|        55|WISCONSIN|         200912|2009|  December|        1|   Drought|      Z|      3|              0|                0|            0|              0|  4|         WI|         003|   Ashland County|      H1|\n|         27029|        27|MINNESOTA|         200912|2009|  December|       23|Heavy Snow|      Z|     29|              0|                0|            0|              0|  4|         MN|         029|Clearwater County|      H1|\n+--------------+----------+---------+---------------+----+----------+---------+----------+-------+-------+---------------+-----------------+-------------+---------------+---+-----------+------------+-----------------+--------+\n\nstormeventsDetailsWithNames: org.apache.spark.sql.DataFrame = [Strm_FIPS_Code: string, STATE_FIPS: string ... 17 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552241282045_-1716391609","id":"20190310-180802_1026006176","dateCreated":"2019-03-10T18:08:02+0000","dateStarted":"2019-03-12T04:23:11+0000","dateFinished":"2019-03-12T04:23:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6163","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=0","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=1"],"interpreterSettingId":"spark2"}}},{"title":"Read in the BLS Employment Census Data","text":"//Read in the employment stats parquet file create earlier and select only a subset of columns relevant to my objective\nval employment_stats = spark.read.parquet(\"hdfs:///tmp/singlefiles/employment_stats.parquet\").select(\"area_fips\",\"own_code\",\"industry_code\",\"agglvl_code\",\"size_code\",\"year\",\"qtr\",\"disclosure_code\",\"qtrly_estabs\",\"month1_emplvl\",\"month2_emplvl\",\"month3_emplvl\")\n","user":"anonymous","dateUpdated":"2019-03-12T02:47:19+0000","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"employment_stats: org.apache.spark.sql.DataFrame = [area_fips: string, own_code: string ... 10 more fields]\n"}]},"apps":[],"jobName":"paragraph_1551755598329_-356647352","id":"20190305-031318_593540716","dateCreated":"2019-03-05T03:13:18+0000","dateStarted":"2019-03-10T18:26:58+0000","dateFinished":"2019-03-10T18:27:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6164"},{"title":"Read and Join in the Industry Code Data (save to Parquet)","text":"//Read in the industry code data\nval industry_codes = spark.read.option(\"header\", \"true\").csv(\"hdfs:///tmp/industry_titles.csv\")\n//Combine this with the employment_stats dataframeval \nval columnsToSum = List(col(\"month1_emplvl\"), col(\"month2_emplvl\"), col(\"month1_emplvl\")) //List the columns to sum \nval employment_stats_with_indus = employment_stats.join(industry_codes, \"industry_code\").withColumn(\"Total_EmpLVL\",columnsToSum.reduce(_ + _))\nemployment_stats_with_indus.write.parquet(\"hdfs:///tmp/singlefiles/employment_stats_with_indus.parquet\")\n//employment_stats_with_indus.limit(5).show()","user":"anonymous","dateUpdated":"2019-03-12T01:35:01+0000","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1552166978816_502196849","id":"20190309-212938_1166744429","dateCreated":"2019-03-09T21:29:38+0000","dateStarted":"2019-03-10T18:44:13+0000","dateFinished":"2019-03-10T18:47:45+0000","status":"ABORT","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6165"},{"title":"Read in the Final (combined) Industry Data","text":"val employment_stats_with_indus = spark.read.parquet(\"hdfs:///tmp/singlefiles/employment_stats_with_indus.parquet\")\nemployment_stats_with_indus.persist()\nemployment_stats_with_indus.show(5)","user":"anonymous","dateUpdated":"2019-03-12T04:23:15+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+---------+--------+-----------+---------+----+---+---------------+------------+-------------+-------------+-------------+--------------------+------------+\n|industry_code|area_fips|own_code|agglvl_code|size_code|year|qtr|disclosure_code|qtrly_estabs|month1_emplvl|month2_emplvl|month3_emplvl|      industry_title|Total_EmpLVL|\n+-------------+---------+--------+-----------+---------+----+---+---------------+------------+-------------+-------------+-------------+--------------------+------------+\n|        62311|    45000|       3|         57|        0|2012|  4|           null|           9|         1004|         1003|         1009|NAICS 62311 Nursi...|      3011.0|\n|       623110|    45000|       3|         58|        0|2012|  1|           null|           9|         1006|          993|          996|NAICS 623110 Nurs...|      3005.0|\n|       623110|    45000|       3|         58|        0|2012|  2|           null|           9|         1011|          999|          996|NAICS 623110 Nurs...|      3021.0|\n|       623110|    45000|       3|         58|        0|2012|  3|           null|           9|          998|         1011|          997|NAICS 623110 Nurs...|      3007.0|\n|       623110|    45000|       3|         58|        0|2012|  4|           null|           9|         1004|         1003|         1009|NAICS 623110 Nurs...|      3011.0|\n+-------------+---------+--------+-----------+---------+----+---+---------------+------------+-------------+-------------+-------------+--------------------+------------+\nonly showing top 5 rows\n\nemployment_stats_with_indus: org.apache.spark.sql.DataFrame = [industry_code: string, area_fips: string ... 12 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552242093879_1418249991","id":"20190310-182133_804659618","dateCreated":"2019-03-10T18:21:33+0000","dateStarted":"2019-03-12T04:23:38+0000","dateFinished":"2019-03-12T04:28:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6166","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=2","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=3"],"interpreterSettingId":"spark2"}}},{"title":"Read in the Acreage Data","text":"//I would like to know which states have the most storms by acre (i.e. 10 storms in a 1000 acre state is .01 per acre, but if the state is only 100 acres then it's a different story)\nval stateAcres = spark.read.option(\"header\", \"true\").csv(\"file:///data/state_acres.csv\").toDF.withColumn(\"STATE\", upper($\"STATE\")).select(\"STATE\",\"State Acres (1000 Acres)\")","user":"anonymous","dateUpdated":"2019-03-12T04:23:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"stateAcres: org.apache.spark.sql.DataFrame = [STATE: string, State Acres (1000 Acres): string]\n"}]},"apps":[],"jobName":"paragraph_1551576693396_1718812877","id":"20190303-013133_926423602","dateCreated":"2019-03-03T01:31:33+0000","dateStarted":"2019-03-12T04:28:24+0000","dateFinished":"2019-03-12T04:28:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6167","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=4"],"interpreterSettingId":"spark2"}}},{"title":"Storm Events Analysis Part 1","text":"//What are the top ten Storm Events cause the most harm to people in the US (deaths + injuries), and which cause the most deaths?\nprintln(\"What are the top ten Storm Events cause the most harm to people?\")\nval harmfulEvents = stormeventsDetailsWithNames.groupBy($\"EVENT_TYPE\").agg(sum($\"INJURIES_DIRECT\"+$\"DEATHS_DIRECT\").as(\"total\"), sum($\"DEATHS_DIRECT\").as(\"total_death\")).sort($\"total\".desc)\nharmfulEvents.show(10)\n\n//Which are the top ten states see the highest count of storm events in the US?\nprintln(\"Which are the top ten states see the highest count of storm events in the US?\")\nval USEventsByState = stormeventsDetailsWithNames.groupBy($\"STATE\").count.sort($\"count\".desc)\nUSEventsByState.show(10)\n\n//What events occur the most in my state?\nprintln(\"What events occur the most in WA?\")\nval waStateEvents = stormeventsDetailsWithNames.filter($\"STATE\" === \"WASHINGTON\").groupBy($\"EVENT_TYPE\").count.sort($\"count\".desc)\nwaStateEvents.show(10)\n\n//Which states have the highest count of events per 1,000 acre in the US?\nprintln(\"Which states have the most events per 1,000 acre?\")\nval USEventsByStateAcres = USEventsByState.join(stateAcres, \"STATE\").withColumn(\"EventsPer1000Acre\", $\"count\" / $\"State Acres (1000 Acres)\").sort($\"EventsPer1000Acre\".desc)\nUSEventsByStateAcres.show(10)\n\n//Which states have the highest count of storm-related injuries per 1,000 acre in the US?\nprintln(\"Which states have the most injuries per acre?\")\nval USHarmByState = stormeventsDetailsWithNames.groupBy($\"STATE\").agg(sum($\"INJURIES_DIRECT\"+$\"DEATHS_DIRECT\").as(\"total\"), sum($\"DEATHS_DIRECT\").as(\"total_death\")).sort($\"total\".desc)\nval USHarmByStateAcres = USHarmByState.join(stateAcres, \"STATE\").withColumn(\"TotalHarmPer1000Acre\", $\"total\" / $\"State Acres (1000 Acres)\").withColumn(\"DeathsPer1000Acre\", $\"total_death\" / $\"State Acres (1000 Acres)\").sort($\"TotalHarmPer1000Acre\".desc)\nUSHarmByStateAcres.show(10)\n\n//What are the top 10 states which have the fewest events per 1,000 acre in the US?\nprintln(\"What are the top 10 states have the least events per 1,000 acre?\")\nval USEventsByBestStateAcres = USEventsByState.join(stateAcres, \"STATE\").withColumn(\"EventsPer1000Acre\", $\"count\" / $\"State Acres (1000 Acres)\").sort($\"EventsPer1000Acre\".asc).limit(10)\nUSEventsByBestStateAcres.persist()\nUSEventsByBestStateAcres.show(10)","user":"anonymous","dateUpdated":"2019-03-12T04:23:28+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"What are the top ten Storm Events cause the most harm to people?\n+-----------------+-------+-----------+\n|       EVENT_TYPE|  total|total_death|\n+-----------------+-------+-----------+\n|          Tornado|84939.0|     4834.0|\n|Thunderstorm Wind|10298.0|      778.0|\n|      Flash Flood| 7187.0|     1177.0|\n|             Heat| 5566.0|     1029.0|\n|        Lightning| 4920.0|      687.0|\n|   Excessive Heat| 2267.0|      216.0|\n|            Flood| 1746.0|      380.0|\n|             Hail| 1298.0|       11.0|\n|   Winter Weather| 1201.0|       68.0|\n|     Winter Storm|  791.0|      105.0|\n+-----------------+-------+-----------+\nonly showing top 10 rows\n\nWhich are the top ten states see the highest count of storm events in the US?\n+--------------+------+\n|         STATE| count|\n+--------------+------+\n|         TEXAS|110800|\n|        KANSAS| 70370|\n|      OKLAHOMA| 58556|\n|      MISSOURI| 49915|\n|          IOWA| 47432|\n|      NEBRASKA| 42848|\n|      ILLINOIS| 41079|\n|       GEORGIA| 37741|\n|NORTH CAROLINA| 36778|\n|     MINNESOTA| 35449|\n+--------------+------+\nonly showing top 10 rows\n\nWhat events occur the most in WA?\n+-----------------+-----+\n|       EVENT_TYPE|count|\n+-----------------+-----+\n|       Heavy Snow|  903|\n|        High Wind|  621|\n|Thunderstorm Wind|  500|\n|            Flood|  386|\n|             Hail|  349|\n|       Heavy Rain|  202|\n|         Wildfire|  159|\n|      Flash Flood|  154|\n|          Tornado|  121|\n|     Winter Storm|  106|\n+-----------------+-----+\nonly showing top 10 rows\n\nWhich states have the most events per 1,000 acre?\n+--------------------+-----+------------------------+------------------+\n|               STATE|count|State Acres (1000 Acres)| EventsPer1000Acre|\n+--------------------+-----+------------------------+------------------+\n|DISTRICT OF COLUMBIA|  717|                      39|18.384615384615383|\n|          NEW JERSEY|14482|                    5258|2.7542791936097375|\n|            DELAWARE| 2822|                    1534|1.8396349413298565|\n|        RHODE ISLAND| 1357|                     788|1.7220812182741116|\n|            MARYLAND|13369|                    7870|1.6987293519695044|\n|       MASSACHUSETTS| 9460|                    5914| 1.599594183293879|\n|            KENTUCKY|35203|                   25863| 1.361133665854696|\n|              KANSAS|70370|                   52660|1.3363083934675275|\n|                OHIO|34835|                   26451|1.3169634418358473|\n|                IOWA|47432|                   36017| 1.316933670211289|\n+--------------------+-----+------------------------+------------------+\nonly showing top 10 rows\n\nWhich states have the most injuries per acre?\n+--------------------+------+-----------+------------------------+--------------------+--------------------+\n|               STATE| total|total_death|State Acres (1000 Acres)|TotalHarmPer1000Acre|   DeathsPer1000Acre|\n+--------------------+------+-----------+------------------------+--------------------+--------------------+\n|DISTRICT OF COLUMBIA| 397.0|       14.0|                      39|  10.179487179487179|   0.358974358974359|\n|       MASSACHUSETTS|2015.0|      136.0|                    5914| 0.34071694284748055|0.022996280013527222|\n|         MISSISSIPPI|7816.0|      667.0|                   30903| 0.25292042843736856|0.021583665016341454|\n|          NEW JERSEY|1243.0|      133.0|                    5258| 0.23640167364016737|0.025294788893115254|\n|             INDIANA|5138.0|      385.0|                   23158| 0.22186717333103032| 0.01662492443216167|\n|                OHIO|5659.0|      361.0|                   26451| 0.21394276208838986|0.013647877206910892|\n|            DELAWARE| 322.0|       27.0|                    1534| 0.20990873533246415|0.017601043024771838|\n|            MISSOURI|8974.0|      702.0|                   44614| 0.20114762182274623|0.015734971085309545|\n|           TENNESSEE|5395.0|      473.0|                   26973| 0.20001482964445927| 0.01753605457309161|\n|            MARYLAND|1386.0|      183.0|                    7870|  0.1761118170266836|0.023252858958068615|\n+--------------------+------+-----------+------------------------+--------------------+--------------------+\nonly showing top 10 rows\n\nWhat are the top 10 states have the least events per 1,000 acre?\n+------------+-----+------------------------+-------------------+\n|       STATE|count|State Acres (1000 Acres)|  EventsPer1000Acre|\n+------------+-----+------------------------+-------------------+\n|      NEVADA| 3709|                   70763| 0.0524143973545497|\n|  WASHINGTON| 4161|                   45208|0.09204123164041762|\n|      OREGON| 5800|                   62140|0.09333762471837785|\n|        UTAH| 5899|                   54339|0.10855923001895508|\n|       IDAHO| 6154|                   53487|0.11505599491465215|\n|  NEW MEXICO| 9963|                   77823| 0.1280212790563201|\n|     WYOMING| 8463|                   62604|0.13518305539582134|\n|     MONTANA|19731|                   94109|0.20966113761701857|\n|       MAINE| 8534|                   21594|0.39520237102898953|\n|NORTH DAKOTA|20970|                   45251|0.46341517314534486|\n+------------+-----+------------------------+-------------------+\n\nharmfulEvents: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EVENT_TYPE: string, total: double ... 1 more field]\nUSEventsByState: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STATE: string, count: bigint]\nwaStateEvents: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EVENT_TYPE: string, count: bigint]\nUSEventsByStateAcres: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STATE: string, count: bigint ... 2 more fields]\nUSHarmByState: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STATE: string, total: double ... 1 more field]\nUSHarmByStateAcres: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STATE: string, total: double ... 4 more fields]\nUSEventsByBestStateAcres: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [S..."}]},"apps":[],"jobName":"paragraph_1552245768864_-1667408460","id":"20190310-192248_716870375","dateCreated":"2019-03-10T19:22:48+0000","dateStarted":"2019-03-12T04:28:28+0000","dateFinished":"2019-03-12T04:29:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6168","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=5","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=6","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=7","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=8","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=9","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=10","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=11","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=12","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=13","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=14","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=15","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=16"],"interpreterSettingId":"spark2"}}},{"title":"Storm Event Analysis Part 2","text":"//Of the top ten states, which county in each state has the fewest events?\nprintln(\"Of the top ten states, which county in each state has the fewest events?\")\nval listGoodStates = USEventsByBestStateAcres.select(\"STATE\").map(_.getString(0)).collect.toList  //Convert the first column to a list we can use as a filter\nval bestStatesDetails = stormeventsDetailsWithNames.filter(($\"STATE\" isin (listGoodStates:_*))) //isin takes a vararg, not a list\n\nval bestCountiesGroup = bestStatesDetails.groupBy($\"STATE\", $\"CZ_NAME\",$\"Strm_FIPS_Code\").count\n\nval bestCountyPerState = bestCountiesGroup.groupBy($\"STATE\").agg(min($\"count\").as(\"count\"))\nval bestCounties = bestCountiesGroup.join(bestCountyPerState, Seq(\"count\",\"STATE\"))\nstormeventsDetailsWithNames.unpersist()\nbestCounties.persist()\n//bestCounties.show()\n//bestCounties.write.parquet(\"hdfs:///tmp/singlefiles/bestCounties.parquet\")\n","user":"anonymous","dateUpdated":"2019-03-12T04:23:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Of the top ten states, which county in each state has the fewest events?\norg.apache.spark.SparkException: Exception thrown in awaitResult:\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n  at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.consume(HashAggregateExec.scala:39)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultFunction(HashAggregateExec.scala:476)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:649)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:165)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:39)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.columnar.InMemoryRelation.buildBuffers(InMemoryRelation.scala:107)\n  at org.apache.spark.sql.execution.columnar.InMemoryRelation.<init>(InMemoryRelation.scala:102)\n  at org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:43)\n  at org.apache.spark.sql.execution.CacheManager$$anonfun$cacheQuery$1.apply(CacheManager.scala:97)\n  at org.apache.spark.sql.execution.CacheManager.writeLock(CacheManager.scala:67)\n  at org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:91)\n  at org.apache.spark.sql.Dataset.persist(Dataset.scala:2907)\n  ... 47 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 1956, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:76)\n\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:107)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:108)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:116)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:237)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1609)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1597)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1596)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1596)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1830)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1779)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1768)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:304)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)\n  at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:97)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n  at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n  at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n  ... 3 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n  at java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:76)\n  at org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:107)\n  at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:108)\n  at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:116)\n  at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:237)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n  at org.apache.spark.scheduler.Task.run(Task.scala:109)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n  ... 3 more\n"}]},"apps":[],"jobName":"paragraph_1552248627404_183863923","id":"20190310-201027_797316047","dateCreated":"2019-03-10T20:10:27+0000","dateStarted":"2019-03-12T04:29:04+0000","dateFinished":"2019-03-12T04:29:17+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:6169","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=17","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=18"],"interpreterSettingId":"spark2"}}},{"title":"Job Prospect Analysis Prep","text":"//Ran these commands in spark-shell -> otherwise the notebook fails.\n//Create a list of the best county's FIPS codes from the StormEvents analysis\nval listBestCounties = bestCounties.select(\"Strm_FIPS_Code\").map(_.getString(0)).collect.toList  //Convert the FIPS code column to a list we can use as a filter\nval bestCountyIndus = employment_stats_with_indus.filter(($\"area_fips\" isin (listBestCounties:_*))) //Filter the County codes in the employment stats DF for just those which match the best counties FIPS codes\n//bestCountyIndus.write.parquet(\"hdfs:///tmp/singlefiles/bestCountyIndus.parquet\")","user":"anonymous","dateUpdated":"2019-03-12T03:43:29+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:27: error: not found: value bestCounties\n       val listBestCounties = bestCounties.select(\"Strm_FIPS_Code\").map(_.getString(0)).collect.toList  //Convert the FIPS code column to a list we can use as a filter\n                              ^\n"}]},"apps":[],"jobName":"paragraph_1552245785918_-225621314","id":"20190310-192305_1458984279","dateCreated":"2019-03-10T19:23:05+0000","dateStarted":"2019-03-12T03:50:50+0000","dateFinished":"2019-03-12T03:50:50+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:6170"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1552358915251_-572815732","id":"20190312-024835_735350736","dateCreated":"2019-03-12T02:48:35+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8305"}],"name":"FinalBD210/Final Project","id":"2E4BKYXQP","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}